{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delay discounting analyis: SOLO models\n",
    "This notebooks gives an overview of using SOLO models for analysing delay discounting data. The SOLO models estimate parameters for each data file independently from the rest. Further, each datafile is processed entirely separately. This is scalable, thus useful for very large datasets. We avoid building _very_ large models with 100's or 1,000's or participants. It can still take time, but the point is we avoid both memory and computational capacity limitations.\n",
    "\n",
    "## Parameter estimation\n",
    "We can do parameter estimation by creating a model instance and calling the `sample_posterior` method while providing the data.\n",
    "\n",
    "## Posterior prediction\n",
    "Once we have a posterior distribution over the parameters given the data, then we can do some posterior predictive model checking by plotting the predicted discount function along with the data. This is done with the `plot_discount_functions_region` method.\n",
    "However we can also use the `df_comparison(models, data)` function in order to plot the data along with posterior predictions of mulitple methods.\n",
    "\n",
    "## Model evaluation\n",
    "Some qualitative or sanity-check model evaluation is done with plotting the posterior predictions (see above). However, we might also want to do some quantitative evaluation.\n",
    "\n",
    "- WAIC\n",
    "- LOO\n",
    "\n",
    "We also calculate the log loss goodness of fit metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, some basic boilerplate setup code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# file handling\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "# data + modelling\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set up plotting preferences\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "SMALL_SIZE = 16\n",
    "MEDIUM_SIZE = 18\n",
    "BIGGER_SIZE = 22\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "# comment out to avoid plotting MANY figures, bloating the notebook\n",
    "# %matplotlib inline\n",
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import toolbox code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# autoreload imported modules. Convenient while I'm developing the code.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from models_solo import *\n",
    "from df_data import *\n",
    "#from df_plotting import df_comparison\n",
    "from model_comparison import *\n",
    "from fitting import fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import\n",
    "The approach we will take is to have two dataframes:\n",
    "- experiment level data. Each row will correspond to one datafile, and will have columns including the filename, but also experiment level information such as condition, or other participant and experiment data.\n",
    "- all response data. This will be one large dataframe which includes _all_ raw response data, all trials for all participants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation step 1: Construct experiment-level information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob('data/non_parametric/*.txt')\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start to build the experiment level dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_data = pd.DataFrame({'filename': files})\n",
    "expt_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will parse the filenames to extract relevant information. This is good when you have put this information into the filenames. But in some situations you might have this experiment level information in a separate file. In that case, you just want to import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename(fname):\n",
    "    \"\"\"Extract initials from provided filename\"\"\"\n",
    "    path, file = os.path.split(fname)\n",
    "    initials = file.split('-')[0]\n",
    "    return initials\n",
    "\n",
    "expt_data['id'] = pd.Series([parse_filename(fname) for fname in files],\n",
    "                           index=expt_data.index)\n",
    "expt_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation step 2: Import raw behavioural data\n",
    "The goal here is to import a list of user-specified raw data files and bundle them up into a pandas dataframe.\n",
    "\n",
    "The approach taken here is to keep this raw code here in the notebook so that the data import process is transparent and easily modifiable by users if their data is in slightly different forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We already have a list of filenames, but we can extract from expt_data\n",
    "filenames = expt_data['filename']\n",
    "\n",
    "raw_data = import_raw_data(filenames)\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we like, we can extract list of unique participant names:\n",
    "\n",
    "    participant_list = list(expt_data.id.unique())\n",
    "    print(participant_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter estimation + Model Comparison\n",
    "\n",
    "First we set up a list of models that we want to examine. Then we 'fit' these models (parameter estimation) then do Bayesian model comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [Coinflip,\n",
    "          Exponential, \n",
    "          Hyperbolic,\n",
    "          HyperboloidA, \n",
    "          HyperboloidB, \n",
    "          ConstantSensitivity, \n",
    "          ExponentialPower,\n",
    "          ExponentialLog,\n",
    "          HyperbolicLog,\n",
    "          DoubleExponential,\n",
    "          BetaDelta,\n",
    "          TradeOff,\n",
    "          ITCH,\n",
    "          DRIFT]\n",
    "\n",
    "# When we do model comparison we want model names in the WAIC/LOO plots. There will be a better solution, but we currenrly implement the workaround.\n",
    "# See https://discourse.pymc.io/t/can-we-add-model-names-when-we-do-model-comparison/935/2 for more.\n",
    "MODEL_NAME_MAP = {\n",
    "    0: \"Coinflip\",\n",
    "    1: \"Exponential\",\n",
    "    2: \"Hyperbolic\",\n",
    "    3: \"Hyperboloid A\",\n",
    "    4: \"Hyperboloid B\",\n",
    "    5: \"Constant Sensitivity\",\n",
    "    6: \"Exponential Power\",\n",
    "    7: \"Exponential Log\",\n",
    "    8: \"Hyperbolic Log\",\n",
    "    9: \"Double Exponential\",\n",
    "    10: \"BetaDelta\",\n",
    "    11: \"TradeOff\",\n",
    "    12: \"ITCH\",\n",
    "    13: \"DRIFT\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = [Coinflip,\n",
    "#           Exponential, \n",
    "#           Hyperbolic]\n",
    "\n",
    "# # When we do model comparison we want model names in the WAIC/LOO plots. There will be a better solution, but we currenrly implement the workaround.\n",
    "# # See https://discourse.pymc.io/t/can-we-add-model-names-when-we-do-model-comparison/935/2 for more.\n",
    "# MODEL_NAME_MAP = {\n",
    "#     0: \"Coinflip\",\n",
    "#     1: \"Exponential\",\n",
    "#     2: \"Hyperbolic\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This will take time to compute! And it will save outputs to the specified directory\n",
    "results = fit(models, raw_data, expt_data, MODEL_NAME_MAP, save_dir='temp_analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now have a series of saved plots. These are all located in the specified `save_dir` which by default equals `'temp'`. In this folder there are model comparison plots, one for each participant. There are also subfolders for each participant, which contains a series of plots for model diagnostics etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO list for fit function**\n",
    "\n",
    "- return model fits from the `fit` function OR save them to disk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
